{{- if .Values.inferenceService.enabled -}}
apiVersion: "serving.kserve.io/v1beta1"
kind: InferenceService
metadata:
  name: {{ include "olist-model-deployment.fullname" . }}
  labels:
    {{- include "olist-model-deployment.labels" . | nindent 4 }}
  annotations:
    keel.sh/policy: "force" 
    keel.sh/trigger: "poll"
    keel.sh/poll-schedule: "@every 5m"

    # # 2. BÃ¡o cho Keel biáº¿t nÃ³ Ä‘ang nÃ¢ng cáº¥p Helm chart nÃ y
    # keel.sh/helm-release: {{ .Release.Name | quote }}
    
    # # 3. BÃ¡o cho Keel biáº¿t pháº£i cáº­p nháº­t giÃ¡ trá»‹ nÃ o trong values.yaml
    # keel.sh/helm-value-path: "image.tag"
    
    # # 4. ğŸ’¡ ÄÃ‚Y LÃ€ KHÃ“A CHÃNH: 
    # # BÃ¡o Keel cáº­p nháº­t giÃ¡ trá»‹ 'image.tag' báº±ng digest cá»§a tag Ä‘Ã³,
    # # thay vÃ¬ chá»‰ dÃ¹ng tÃªn tag.
    # keel.sh/update-strategy: "digest"
    serving.knative.dev/revision-timeout-seconds: "1800"
spec:
  predictor:
    # === Cáº¥u hÃ¬nh Scale ===
    # Äá»c tá»« values.yaml
    minReplicas: {{ .Values.autoscaling.minReplicas }} # Sáº½ lÃ  0
    maxReplicas: {{ .Values.autoscaling.maxReplicas }}
    
    # === Cáº¥u hÃ¬nh Service Account ===
    serviceAccountName: {{ include "olist-model-deployment.serviceAccountName" . }}
    
    # === Äá»‹nh nghÄ©a Container Model ===
    containers:
    - name: {{ .Chart.Name }} # TÃªn container (KServe yÃªu cáº§u)
      image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
      imagePullPolicy: {{ .Values.image.pullPolicy }}
      ports:
        # Port mÃ  container cá»§a báº¡n Ä‘ang cháº¡y
        - containerPort: {{ .Values.containerPort }}
          name: http1 # Knative yÃªu cáº§u port pháº£i cÃ³ tÃªn
          protocol: TCP
      env:
      - name: MLSERVER_HTTP_PORT
        value: "{{ .Values.containerPort }}" # Sáº½ láº¥y 8080 tá»« values.yaml
      - name: MLSERVER_HOST
        value: "0.0.0.0"
      - name: MLSERVER_PARALLEL_WORKERS
        value: "0"
      - name: MLFLOW_ENV_MANAGER
        value: "{{ .Values.mlflowEnvManager }}"

      # Ghi Ä‘Ã¨ Entrypoint vÃ  Command (giá»‘ng há»‡t lá»‡nh Docker cá»§a báº¡n)
      command: ["/bin/bash"]
      args:
      - "-c"
      - |
        echo '{"name": "{{ .Values.modelName }}", "implementation": "mlserver_mlflow.MLflowRuntime"}' > /opt/ml/model/model-settings.json
        echo "--- model-settings.json created (name: {{ .Values.modelName }}), starting MLServer... ---"
        {{ .Values.mlserverPath }} start /opt/ml/model
      # ğŸ“ˆ Cáº¥u hÃ¬nh target cho HPA (náº¿u muá»‘n)
      # ÄÃ¢y lÃ  cÃ¡ch KServe Ã¡p dá»¥ng target CPU/Memory
      resources:
        requests:
          # Äáº·t requests tháº¥p Ä‘á»ƒ dá»… scale-to-zero
          cpu: "100m" 
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "1Gi"

      {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}
        # ThÃªm annotation Ä‘á»ƒ KPA biáº¿t má»¥c tiÃªu CPU
      annotations:
        autoscaling.knative.dev/metric: "cpu"
        autoscaling.knative.dev/target: "{{ .Values.autoscaling.targetCPUUtilizationPercentage }}" # Target 80%
      {{- end }}
{{- end -}}
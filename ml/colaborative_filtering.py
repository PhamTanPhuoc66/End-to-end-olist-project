import os
import mlflow
import pandas as pd
import numpy as np
from databricks import sql
from dotenv import load_dotenv
import xgboost
import sklearn
import imblearn
import IPython
import scipy
import joblib
import cloudpickle
import surprise 
conda_env_dict = {
    'name': 'mlflow-env-with-gcc', # T√™n m√¥i tr∆∞·ªùng
    'channels': ['conda-forge', 'defaults'], # K√™nh
    'dependencies': [
        'python=3.12', # Phi√™n b·∫£n Python

        # ----- Th√™m GCC -----
        'gcc_linux-64', # G√≥i GCC cho Linux x64 t·ª´ conda-forge

        # ----- C√°c g√≥i ch√≠nh -----
        # S·ª≠ d·ª•ng f-string ƒë·ªÉ l·∫•y phi√™n b·∫£n t·ª´ c√°c th∆∞ vi·ªán ƒë√£ import
        'mlflow',
        f'cloudpickle={cloudpickle.__version__}',
        f'joblib={joblib.__version__}',
        f'pandas={pd.__version__}',
        f'numpy={np.__version__}',
        f'scikit-learn={sklearn.__version__}',
        f'imbalanced-learn={imblearn.__version__}',
        'ipython',
        f'scikit-surprise={surprise.__version__}',
        f'scipy={scipy.__version__}',
        'threadpoolctl', # C√≥ th·ªÉ kh√¥ng c·∫ßn ghi t∆∞·ªùng minh

        # ----- Lu√¥n bao g·ªìm pip -----
        'pip'
    ],
    # ----- Ph·∫ßn pip (th∆∞·ªùng tr·ªëng n·∫øu c√°c g√≥i tr√™n c√≥ tr√™n conda) -----
    # 'pip': [
    #    'some-package-only-on-pip'
    # ]
}
SURPRISE_MODEL_TAGS = {"build_env": "needs_gcc"}

load_dotenv()
database = "olist_dataset"
goldSche=database+ ".gold"
lookupGeo=database+".silver"+ ".silver_olist_geolocation"
lookupCateName=database+".silver"+ ".silver_product_category_name_translation"
connection = sql.connect(
                        server_hostname = "dbc-214ffec4-c1f2.cloud.databricks.com",
                        http_path = "/sql/1.0/warehouses/8753fa7395d762fe",
                        access_token = os.environ.get("DATABRICKS_TOKEN"))


query=f"""
WITH 
-- 1. L·∫•y d·ªØ li·ªáu t∆∞∆°ng t√°c c·ªët l√µi gi·ªØa User v√† Item
user_item_interactions AS (
    SELECT
        c.customer_unique_id,
        oi.product_key,
        COUNT(oi.order_id) AS purchase_count, -- S·ªë l·∫ßn user mua s·∫£n ph·∫©m n√†y
        SUM(oi.price) AS total_spent_on_product, -- T·ªïng ti·ªÅn user chi cho s·∫£n ph·∫©m n√†y
        AVG(oi.price) AS avg_price_for_product,
        SUM(oi.freight_value) AS total_freight_on_product,
        MIN(d.full_date) AS first_purchase_date,
        MAX(d.full_date) AS last_purchase_date
    FROM {goldSche}.fact_order_items oi
    JOIN {goldSche}.dim_olist_customers c ON oi.customer_key = c.customer_key
    JOIN {goldSche}.dim_olist_orders o ON oi.order_key = o.order_key
    JOIN {goldSche}.dim_date d ON o.order_purchase_date_key = d.date_key
    GROUP BY c.customer_unique_id, oi.product_key
),

-- 2. L·∫•y ƒëi·ªÉm review (explicit feedback) cho t·ª´ng c·∫∑p (user, item)
-- Gi·∫£ ƒë·ªãnh r·∫±ng review c·ªßa m·ªôt ƒë∆°n h√†ng √°p d·ª•ng cho t·∫•t c·∫£ s·∫£n ph·∫©m trong ƒë∆°n h√†ng ƒë√≥.
product_reviews AS (
    SELECT 
        c.customer_unique_id,
        oi.product_key,
        AVG(r.score) AS avg_review_score_by_user -- ƒêi·ªÉm review trung b√¨nh user d√†nh cho s·∫£n ph·∫©m
    FROM {goldSche}.fact_reviews r
    JOIN {goldSche}.fact_order_items oi ON r.order_key = oi.order_key
    JOIN {goldSche}.dim_olist_customers c ON r.customer_key = c.customer_key
    GROUP BY c.customer_unique_id, oi.product_key
),

-- 3. X√¢y d·ª±ng c√°c feature t·ªïng h·ª£p cho m·ªói User
user_features AS (
    SELECT
        c.customer_unique_id,
        COUNT(DISTINCT o.order_id) AS total_orders,
        SUM(oi.price) AS total_monetary,
        AVG(r.score) AS avg_overall_review_score, -- ƒêi·ªÉm review trung b√¨nh c·ªßa user tr√™n m·ªçi s·∫£n ph·∫©m
        COUNT(DISTINCT p.product_category_name) AS distinct_categories_purchased,
        c.customer_city,
        c.customer_state
    FROM {goldSche}.fact_order_items oi
    JOIN {goldSche}.dim_olist_customers c ON oi.customer_key = c.customer_key
    JOIN {goldSche}.dim_olist_orders o ON oi.order_key = o.order_key
    JOIN {goldSche}.dim_olist_products p ON oi.product_key = p.product_key
    LEFT JOIN {goldSche}.fact_reviews r ON o.order_key = r.order_key
    GROUP BY c.customer_unique_id, c.customer_city, c.customer_state
),

-- 4. X√¢y d·ª±ng c√°c feature t·ªïng h·ª£p cho m·ªói Product (Item)
product_features AS (
    SELECT
        oi.product_key,
        AVG(oi.price) AS avg_product_price, -- Gi√° b√°n trung b√¨nh c·ªßa s·∫£n ph·∫©m
        COUNT(DISTINCT c.customer_unique_id) AS num_unique_purchasers, -- S·ªë l∆∞·ª£ng ng∆∞·ªùi mua kh√°c nhau
        COUNT(oi.order_id) AS total_sales_count, -- T·ªïng s·ªë l·∫ßn ƒë∆∞·ª£c b√°n
        AVG(r.score) AS avg_product_review_score -- ƒêi·ªÉm review trung b√¨nh c·ªßa s·∫£n ph·∫©m tr√™n m·ªçi user
    FROM {goldSche}.fact_order_items oi
    JOIN {goldSche}.dim_olist_customers c ON oi.customer_key = c.customer_key
    LEFT JOIN {goldSche}.fact_reviews r ON oi.order_key = r.order_key
    GROUP BY oi.product_key
)

-- 5. K·∫øt h·ª£p t·∫•t c·∫£ l·∫°i ƒë·ªÉ t·∫°o ra b·∫£ng ph√¢n t√≠ch cu·ªëi c√πng
SELECT
    -- Interaction Keys
    uii.customer_unique_id,
    uii.product_key,
    p.product_category_name,

    -- Interaction Features (Implicit Feedback)
    uii.purchase_count,
    uii.total_spent_on_product,
    uii.avg_price_for_product,
    uii.total_freight_on_product,
    uii.first_purchase_date,
    uii.last_purchase_date,
    
    -- Explicit Feedback
    pr.avg_review_score_by_user,

    -- User Features
    uf.total_orders AS user_total_orders,
    uf.total_monetary AS user_total_monetary,
    uf.avg_overall_review_score AS user_avg_overall_review_score,
    uf.distinct_categories_purchased AS user_distinct_categories_purchased,
    uf.customer_city,
    uf.customer_state,
    
    -- Product (Item) Features
    pf.avg_product_price,
    pf.num_unique_purchasers AS product_num_unique_purchasers,
    pf.total_sales_count AS product_total_sales_count,
    pf.avg_product_review_score AS product_avg_review_score,
    p.product_weight_g,
    p.product_length_cm,
    p.product_height_cm,
    p.product_width_cm

FROM user_item_interactions uii
LEFT JOIN product_reviews pr ON uii.customer_unique_id = pr.customer_unique_id AND uii.product_key = pr.product_key
LEFT JOIN user_features uf ON uii.customer_unique_id = uf.customer_unique_id
LEFT JOIN product_features pf ON uii.product_key = pf.product_key
JOIN {goldSche}.dim_olist_products p ON uii.product_key = p.product_key
ORDER BY uii.customer_unique_id, uii.purchase_count DESC;
"""
df = pd.read_sql(query, connection)
df.describe()
df.info()
connection.close()


# X·ª≠ l√Ω c√°c c·ªôt review b·ªã thi·∫øu
review_cols = ['avg_review_score_by_user', 'user_avg_overall_review_score', 'product_avg_review_score']
for col in review_cols:
    median_value = df[col].median()
    df[col].fillna(median_value, inplace=True)
    print(f"ƒê√£ ƒëi·ªÅn c√°c gi√° tr·ªã thi·∫øu trong c·ªôt '{col}' b·∫±ng median: {median_value:.2f}")

# X·ª≠ l√Ω c√°c c·ªôt thu·ªôc t√≠nh s·∫£n ph·∫©m b·ªã thi·∫øu (ch·ªâ c√≥ 1 d√≤ng)
# Ta s·∫Ω x√≥a c√°c d√≤ng c√≥ b·∫•t k·ª≥ gi√° tr·ªã n√†o b·ªã thi·∫øu trong c√°c c·ªôt n√†y
product_physic_cols = ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']
initial_rows = len(df)
df.dropna(subset=product_physic_cols, inplace=True)
rows_dropped = initial_rows - len(df)

print(f"\nƒê√£ x√≥a {rows_dropped} d√≤ng c√≥ gi√° tr·ªã thi·∫øu ·ªü thu·ªôc t√≠nh v·∫≠t l√Ω c·ªßa s·∫£n ph·∫©m.")

# Chuy·ªÉn ƒë·ªïi c√°c c·ªôt ng√†y th√°ng sang ki·ªÉu datetime
date_cols = ['first_purchase_date', 'last_purchase_date']

for col in date_cols:
    df[col] = pd.to_datetime(df[col])
    print(f"ƒê√£ chuy·ªÉn ƒë·ªïi c·ªôt '{col}' sang ki·ªÉu datetime.")

# Ki·ªÉm tra l·∫°i ki·ªÉu d·ªØ li·ªáu c·ªßa c√°c c·ªôt n√†y
print("\nKi·ªÉu d·ªØ li·ªáu sau khi chuy·ªÉn ƒë·ªïi:")
print(df[date_cols].dtypes)

# ƒê·∫øm s·ªë l∆∞·ª£ng d√≤ng b·ªã tr√πng l·∫∑p
duplicate_rows = df.duplicated().sum()
print(f"T√¨m th·∫•y {duplicate_rows} d√≤ng d·ªØ li·ªáu b·ªã tr√πng l·∫∑p.")

# X√≥a c√°c d√≤ng b·ªã tr√πng l·∫∑p n·∫øu c√≥
if duplicate_rows > 0:
    df.drop_duplicates(inplace=True)
    print("ƒê√£ x√≥a c√°c d√≤ng b·ªã tr√πng l·∫∑p.")
    
    
# Cell Code Ho√†n Ch·ªânh - ƒê√≥ng g√≥i th√†nh Class Tuner Linh ho·∫°t

# ===================================================================
# 1. IMPORTS V√Ä C√ÄI ƒê·∫∂T
# ===================================================================
import pandas as pd
import numpy as np
import os
import tempfile
import mlflow
import surprise
import gc
from surprise import SVD, SVDpp, NMF, SlopeOne,KNNBasic, Dataset, Reader, accuracy
from surprise.model_selection import train_test_split, GridSearchCV
from joblib import Parallel, delayed
from typing import Dict, Optional, Any
from ultils import RecSysModelTuner
# C·∫•u h√¨nh MLflow
mlflow.set_tracking_uri("http://172.17.0.1:5000")

# Gi·∫£ ƒë·ªãnh b·∫°n ƒë√£ c√≥ m·ªôt DataFrame t√™n l√† 'df' ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† t·∫£i s·∫µn

# ===================================================================
# 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU (Th·ª±c hi·ªán m·ªôt l·∫ßn ·ªü ngo√†i)
# ===================================================================
print("Chu·∫©n b·ªã d·ªØ li·ªáu cho Surprise...")
cf_df = df[['customer_unique_id', 'product_key', 'avg_review_score_by_user']].copy()
cf_df.columns = ['userID', 'itemID', 'rating']
cf_df.dropna(subset=['rating'], inplace=True)
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(cf_df, reader)
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)
full_trainset = data.build_full_trainset()
input_example = cf_df[['userID', 'itemID']].head(5)
print("‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng!")




# ===================================================================
# 4. FLOW TH·ª¨ NGHI·ªÜM CH√çNH (S·ª¨ D·ª§NG CLASS M·ªöI)
# ===================================================================
experiment_name = "Collaborative_Filtering_Class_Based_Flow"
mlflow.set_experiment(experiment_name)

with mlflow.start_run(run_name="Main_CF_Experiment") as parent_run:
    mlflow.set_tag("project", "Flexible Tuning of CF Models")
    print(f"üöÄ B·∫Øt ƒë·∫ßu Parent Run: {parent_run.info.run_id}")

    # --- Th·ª≠ nghi·ªám SVD ---
    param_grid_svd = {'n_factors': [50, 100], 'n_epochs': [20], 'lr_all': [0.005], 'reg_all': [0.02], 'random_state': [42]}
    svd_tuner = RecSysModelTuner(SVD, "SVD_Tuning", "Matrix Factorization", "svd-recommender-tuned", param_grid=param_grid_svd, conda_env=conda_env_dict,tags=SURPRISE_MODEL_TAGS)
    svd_tuner.run(data, full_trainset, trainset, testset, input_example)
    
    # --- Th·ª≠ nghi·ªám SVD++ ---
    param_grid_svdpp = {'n_factors': [20, 50], 'n_epochs': [20], 'lr_all': [0.007], 'reg_all': [0.02], 'random_state': [42]}
    svdpp_tuner = RecSysModelTuner(SVDpp, "SVDpp_Tuning", "Matrix Factorization", "svdpp-recommender-tuned", param_grid=param_grid_svdpp,conda_env=conda_env_dict,tags=SURPRISE_MODEL_TAGS)
    svdpp_tuner.run(data, full_trainset, trainset, testset, input_example)
    
    # --- Th·ª≠ nghi·ªám NMF ---
    param_grid_nmf = {'n_factors': [15, 30], 'n_epochs': [50], 'reg_pu': [0.06], 'reg_qi': [0.06], 'random_state': [42]}
    nmf_tuner = RecSysModelTuner(NMF, "NMF_Tuning", "Matrix Factorization", "nmf-recommender-tuned", param_grid=param_grid_nmf,conda_env=conda_env_dict,tags=SURPRISE_MODEL_TAGS)
    nmf_tuner.run(data, full_trainset, trainset, testset, input_example)

    # # --- Th·ª≠ nghi·ªám SlopeOne (kh√¥ng tuning) ---
    SAMPLE_FRAC = 0.15 # L·∫•y 15% d·ªØ li·ªáu
    sampled_cf_df = cf_df.sample(frac=SAMPLE_FRAC, random_state=42)
    
    # T·∫°o l·∫°i c√°c ƒë·ªëi t∆∞·ª£ng surprise t·ª´ d·ªØ li·ªáu m·∫´u
    data_sampled = Dataset.load_from_df(sampled_cf_df, reader)
    trainset_sampled, testset_sampled = train_test_split(data_sampled, test_size=0.2, random_state=42)
    full_trainset_sampled = data_sampled.build_full_trainset()
    print(f"‚úÖ D·ªØ li·ªáu m·∫´u ƒë√£ s·∫µn s√†ng v·ªõi {len(sampled_cf_df)} t∆∞∆°ng t√°c.")
    
    slopeone_tuner = RecSysModelTuner(SlopeOne, "SlopeOne_Run_Sampled", "Item-Based", "slopeone-recommender-fixed",conda_env=conda_env_dict,tags=SURPRISE_MODEL_TAGS)
    # Ch·∫°y tuner tr√™n b·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l·∫•y m·∫´u
    slopeone_tuner.run(
        data_sampled, 
        full_trainset_sampled, 
        trainset_sampled, 
        testset_sampled, 
        input_example,
        # Log th√™m tham s·ªë v·ªÅ vi·ªác l·∫•y m·∫´u
        sample_fraction=SAMPLE_FRAC,
    )
    # --- Th·ª≠ nghi·ªám k-NN tr√™n d·ªØ li·ªáu m·∫´u ---
    knn_tuner = RecSysModelTuner(KNNBasic, "KNN_ItemBased_Sampled_Run", "Neighborhood-Based", "knn-item-recommender-sampled",conda_env=conda_env_dict,tags=SURPRISE_MODEL_TAGS)
    knn_tuner.run(data_sampled, full_trainset_sampled, trainset_sampled, testset_sampled, input_example, sample_fraction=SAMPLE_FRAC)


del df
gc.collect()
print(f"\nüèÅ Ho√†n th√†nh Parent Run.")